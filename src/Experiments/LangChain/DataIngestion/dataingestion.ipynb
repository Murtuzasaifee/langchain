{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Ingestion- Documentloaders\n",
    "\n",
    "https://python.langchain.com/v0.2/docs/integrations/document_loaders/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
      "  context: RunnableLambda(format_docs)\n",
      "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
      "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\n    Answer the following question based only on the provided context:\\n    <context>\\n    {context}\\n    </context>\\n    '), additional_kwargs={})])\n",
      "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x10e3ee030>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x10e3ec950>, root_client=<openai.OpenAI object at 0x10e3c7860>, root_async_client=<openai.AsyncOpenAI object at 0x10e3ee090>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
      "| StrOutputParser() kwargs={} config={'run_name': 'stuff_documents_chain'} config_factories=[]\n",
      "\n",
      "\n",
      "Answer:\n",
      "The provided context appears to be an excerpt from a speech that emphasizes the necessity of entering a war to protect democracy and political liberty, without selfish intentions or desires for conquest. The speaker is addressing Congress, acknowledging the distressing duty of leading a peaceful nation into war for the sake of securing the rights and freedoms of all nations and establishing fair and peaceful global relations. The speaker expresses confidence in conducting the war without rancor, as the fight is against an irresponsible government rather than its people, and emphasizes a commitment to universal rights and fair play. This task is portrayed as a noble endeavor that requires devotion and sacrifice. The context likely reflects sentiments from a historical wartime speech, such as those given during World War I.\n"
     ]
    }
   ],
   "source": [
    "## Text Loader - RAG\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "loader=TextLoader('speech.txt')\n",
    "loader\n",
    "\n",
    "text_documents=loader.load()\n",
    "text_documents\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(text_documents)\n",
    "documents\n",
    "\n",
    "## OpenAI Embeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "## Store in the FAISS Vector Store\n",
    "vector_store = FAISS.from_documents(documents,embeddings)\n",
    "\n",
    "## Initializing the Model\n",
    "llm = ChatOpenAI(model='gpt-4o')\n",
    "\n",
    "## Creating the Chat Prompt Template\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Answer the following question based only on the provided context:\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "## Creating the Document Chain\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm,prompt)\n",
    "print(document_chain)\n",
    "\n",
    "## Create Retriver Chain to add Vectore Store to the chain as an interface\n",
    "retriever = vector_store.as_retriever()\n",
    "retrieval_chain=create_retrieval_chain(retriever,document_chain)\n",
    "retrieval_chain\n",
    "\n",
    "## Run the RAG Pipeline\n",
    "\n",
    "result=retrieval_chain.invoke({\"input\":\"safe for democracy\"})\n",
    "print(\"\\n\\nAnswer:\")\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
      "  context: RunnableLambda(format_docs)\n",
      "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
      "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\n    Answer the following question based only on the provided context:\\n    <context>\\n    {context}\\n    </context>\\n    '), additional_kwargs={})])\n",
      "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x10f670830>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x10f672870>, root_client=<openai.OpenAI object at 0x10f128920>, root_async_client=<openai.AsyncOpenAI object at 0x10f6709e0>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
      "| StrOutputParser() kwargs={} config={'run_name': 'stuff_documents_chain'} config_factories=[]\n",
      "\n",
      "\n",
      "Answer:\n",
      "Based on the provided context, what are some of the topics covered in the Python programming modules of this course?\n",
      "\n",
      "**Answer:**\n",
      "\n",
      "The Python programming modules in the course cover a variety of topics:\n",
      "\n",
      "- **Module 1: Python Foundations**\n",
      "  - Introduction to Python, comparison with other programming languages.\n",
      "  - Python objects including numbers, booleans, and strings.\n",
      "  - Data structures and operations, container objects and mutability.\n",
      "  - Operators, operator precedence, and associativity.\n",
      "  - Control flow, including conditional statements, loops, break, and continue statements.\n",
      "  - String manipulation, including the basics of string objects and inbuilt string methods.\n",
      "\n",
      "- **Module 2: Advanced Python Programming**\n",
      "  - Object-Oriented Programming (OOP) including class creation, inheritance, polymorphism, encapsulation, and abstraction.\n",
      "  - Use of decorators, class methods, static methods, special (magic/dunder) methods, and property decorators such as getters, setters, and delete methods.\n",
      "  - File handling and logging, including reading and writing files, buffered read/write operations, and logging/debugging techniques.\n",
      "  - Modules and exception handling, including importing and using modules.\n"
     ]
    }
   ],
   "source": [
    "## PDf File - RAG\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader=PyPDFLoader('syllabus.pdf')\n",
    "docs=loader.load()\n",
    "docs\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "documents\n",
    "\n",
    "## OpenAI Embeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "## Store in the FAISS Vector Store\n",
    "vector_store = FAISS.from_documents(documents,embeddings)\n",
    "\n",
    "## Initializing the Model\n",
    "llm = ChatOpenAI(model='gpt-4o')\n",
    "\n",
    "## Creating the Chat Prompt Template\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Answer the following question based only on the provided context:\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "## Creating the Document Chain\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm,prompt)\n",
    "print(document_chain)\n",
    "\n",
    "## Create Retriver Chain to add Vectore Store to the chain as an interface\n",
    "retriever = vector_store.as_retriever()\n",
    "retrieval_chain=create_retrieval_chain(retriever,document_chain)\n",
    "retrieval_chain\n",
    "\n",
    "## Run the RAG Pipeline\n",
    "\n",
    "result=retrieval_chain.invoke({\"input\":\"Python Foundation\"})\n",
    "print(\"\\n\\nAnswer:\")\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
      "  context: RunnableLambda(format_docs)\n",
      "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
      "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\n    Answer the following question based only on the provided context:\\n    <context>\\n    {context}\\n    </context>\\n    '), additional_kwargs={})])\n",
      "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x10f7bf470>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x10e2ecc80>, root_client=<openai.OpenAI object at 0x10fb9bda0>, root_async_client=<openai.AsyncOpenAI object at 0x12817ae10>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
      "| StrOutputParser() kwargs={} config={'run_name': 'stuff_documents_chain'} config_factories=[]\n",
      "\n",
      "\n",
      "Answer:\n",
      "Based on the provided context, the LLM+P approach by Liu et al. (2023) is a method that involves using an external classical planner for long-horizon planning. It uses the Planning Domain Definition Language (PDDL) as an intermediary to describe the problem. The process encompasses three main steps: (1) the LLM translates the problem into a \"Problem PDDL\", (2) a classical planner is used to generate a plan based on a \"Domain PDDL\", and (3) the LLM translates the PDDL plan back into natural language. This method relies on having domain-specific PDDL and a suitable planner, which is common in certain robotics applications but may not be applicable in many other domains. On the other hand, self-reflection is highlighted as an important aspect for autonomous agents to iteratively improve by refining decision-making processes and correcting past mistakes, playing a crucial role especially in real-world tasks that involve trial and error. Reflexion, as described by Shinn & Labash (2023), is a framework that incorporates self-reflection and dynamic memory to enhance reasoning skills.\n"
     ]
    }
   ],
   "source": [
    "## Web based loader - RAG\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "loader=WebBaseLoader(web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "                     bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "                         class_=(\"post-title\",\"post-content\",\"post-header\")\n",
    "                     ))\n",
    "                     )\n",
    "web_docs=loader.load()\n",
    "web_docs\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(web_docs)\n",
    "documents\n",
    "\n",
    "## OpenAI Embeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "## Store in the FAISS Vector Store\n",
    "vector_store = FAISS.from_documents(documents,embeddings)\n",
    "\n",
    "## Initializing the Model\n",
    "llm = ChatOpenAI(model='gpt-4o')\n",
    "\n",
    "## Creating the Chat Prompt Template\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Answer the following question based only on the provided context:\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "## Creating the Document Chain\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm,prompt)\n",
    "print(document_chain)\n",
    "\n",
    "## Create Retriver Chain to add Vectore Store to the chain as an interface\n",
    "retriever = vector_store.as_retriever()\n",
    "retrieval_chain=create_retrieval_chain(retriever,document_chain)\n",
    "retrieval_chain\n",
    "\n",
    "## Run the RAG Pipeline\n",
    "\n",
    "result=retrieval_chain.invoke({\"input\":\"Self-Reflection\"})\n",
    "print(\"\\n\\nAnswer:\")\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
      "  context: RunnableLambda(format_docs)\n",
      "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
      "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\n    Answer the following question based only on the provided context:\\n    <context>\\n    {context}\\n    </context>\\n    '), additional_kwargs={})])\n",
      "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x129a419a0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x129a40440>, root_client=<openai.OpenAI object at 0x1297ffaa0>, root_async_client=<openai.AsyncOpenAI object at 0x129a419d0>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
      "| StrOutputParser() kwargs={} config={'run_name': 'stuff_documents_chain'} config_factories=[]\n",
      "\n",
      "\n",
      "Answer:\n",
      "The text describes the Transformer model, a neural sequence transduction model based entirely on attention mechanisms, dispensing with recurrent and convolutional layers. It achieves state-of-the-art results in machine translation tasks, specifically on the WMT 2014 English-to-German and English-to-French tasks. The Transformer is more parallelizable and faster to train than previous models.\n",
      "\n",
      "The architecture is based on the encoder-decoder structure. Both the encoder and decoder consist of six identical layers, primarily using multi-head self-attention mechanisms and position-wise fully connected feed-forward networks. Residual connections and layer normalization are employed for each sub-layer, and the model's outputs have a dimension of 512.\n",
      "\n",
      "The text suggests the Transformer generalizes well to other tasks beyond translation and discusses potential extensions to different modalities and efficient handling of large inputs and outputs.\n"
     ]
    }
   ],
   "source": [
    "## Arxiv - RAG\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "arxiv_docs = ArxivLoader(query=\"1706.03762\", load_max_docs=2).load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(arxiv_docs)\n",
    "documents\n",
    "\n",
    "## OpenAI Embeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "## Store in the FAISS Vector Store\n",
    "vector_store = FAISS.from_documents(documents,embeddings)\n",
    "\n",
    "## Initializing the Model\n",
    "llm = ChatOpenAI(model='gpt-4o')\n",
    "\n",
    "## Creating the Chat Prompt Template\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Answer the following question based only on the provided context:\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "## Creating the Document Chain\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm,prompt)\n",
    "print(document_chain)\n",
    "\n",
    "## Create Retriver Chain to add Vectore Store to the chain as an interface\n",
    "retriever = vector_store.as_retriever()\n",
    "retrieval_chain=create_retrieval_chain(retriever,document_chain)\n",
    "retrieval_chain\n",
    "\n",
    "## Run the RAG Pipeline\n",
    "\n",
    "result=retrieval_chain.invoke({\"input\":\"Encoder\"})\n",
    "print(\"\\n\\nAnswer:\")\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
      "  context: RunnableLambda(format_docs)\n",
      "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
      "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\n    Answer the following question based only on the provided context:\\n    <context>\\n    {context}\\n    </context>\\n    '), additional_kwargs={})])\n",
      "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x129c1ac30>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x129c1b890>, root_client=<openai.OpenAI object at 0x10e3c5a90>, root_async_client=<openai.AsyncOpenAI object at 0x129c1ac90>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
      "| StrOutputParser() kwargs={} config={'run_name': 'stuff_documents_chain'} config_factories=[]\n",
      "\n",
      "\n",
      "Answer:\n",
      "What advancements enabled the AI boom of generative AI systems in the 2020s?\n"
     ]
    }
   ],
   "source": [
    "## Wikipedia - RAG\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "\n",
    "wiki_docs = WikipediaLoader(query=\"Generative AI\", load_max_docs=2).load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(wiki_docs)\n",
    "documents\n",
    "\n",
    "## OpenAI Embeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "## Store in the FAISS Vector Store\n",
    "vector_store = FAISS.from_documents(documents,embeddings)\n",
    "\n",
    "## Initializing the Model\n",
    "llm = ChatOpenAI(model='gpt-4o')\n",
    "\n",
    "## Creating the Chat Prompt Template\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Answer the following question based only on the provided context:\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "## Creating the Document Chain\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm,prompt)\n",
    "print(document_chain)\n",
    "\n",
    "## Create Retriver Chain to add Vectore Store to the chain as an interface\n",
    "retriever = vector_store.as_retriever()\n",
    "retrieval_chain=create_retrieval_chain(retriever,document_chain)\n",
    "retrieval_chain\n",
    "\n",
    "## Run the RAG Pipeline\n",
    "\n",
    "result=retrieval_chain.invoke({\"input\":\"What is generative AI\"})\n",
    "print(\"\\n\\nAnswer:\")\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
